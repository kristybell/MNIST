{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network for MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply all the knowledge from the lectures in this section to write a deep neural network. The problem we have chosen is referred to as the \"Hello World\" of deep learning because for most students it is the first deep learning algorithm they see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional neural networks (CNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1,2,3,4,5,6,7,8,9), this is a classification problem with 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal would be to build a neural network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Relevant Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING DATA\n",
    "# 'tfds.load(name)' loads a dataset from TensorFlow datasets\n",
    "# 'as_supervised' = True, loads the data in a 2-tuple structure [input,target]\n",
    "# 'with_info' = True, provides a tuple containing info about version, features, # samples of the dataset\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "# extract the train and test data\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "# mnsit dataset module does not have a validation dataset\n",
    "\n",
    "# take 10% of the training dataset to serve as validation\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "# not sure if the results will come out as an integer, thus must state this limit so it does\n",
    "#'tf.cast(x,dtype)' converts a variable into a diven data tyoe\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# do the same for the test samples\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "# normally, we'd like to scale our data in some way to make the results more numerically stable (e.g. inputs between 0 and 1)\n",
    "def scale(image,label):\n",
    "    image = tf.cast(image, tf.float32)  # to ensure it is a float\n",
    "    image /= 255.   # the '.' at the end ensures it is a float\n",
    "    return image, label\n",
    "\n",
    "# 'dataset.map(*function*)' -> applies a custom transformation to a given dataset; it takes as input function which determines the transformation\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "# Shuffling -> keeping the same information but in a different order\n",
    "# - necessary so that the batching has a variety of random data, as the data typically is listed in ascending order of the targets\n",
    "\n",
    "# in cases of dealing with enormous datasets, we can't shuffle all data at once\n",
    "BUFFER_SIZE = 10000   # shuffle the dataset with 10,000 data at a time\n",
    "\n",
    "# Note: if buffer_size = 1, no shuffling will actually happen\n",
    "#       if buffer_size >= num_samples, shuffling will happen at once (uniformly)\n",
    "#       if 1 < buffer_size < num_samples, we will be optimizing the computational power\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# to create validation data extract the from the data the same amount as there are samples \n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "# to create train data by extracting all data except for the validation data\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# set batch size; batch size=1 -> stochastic gradient descent (SGD); batchsize = # samples -> (single batch) GD\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 'dataset.batch(batch_size)' -> a method that combines the consecutive elements of a dataset into batches\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "# since we are only forward propagating on the validation data and not backpropagating, it is not necessary to create a batchsize\n",
    "# when batching we find the AVERAGE loss\n",
    "# BUT, the model expects the validation and test data in batch form too\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "# the MNIST data is iterable and in 2-tuple format (as_supervised = True)\n",
    "# thus, must extract and convert the validation inputs and targets appropriately\n",
    "# 'iter()' creates an object which can be iterated one element at a time (i.e. in a for loop or while loop)\n",
    "# 'next()' loads the next element of an iterable object\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 784 inputs --> 784 input layers\n",
    "# 10 digits --> 10 output layers\n",
    "# 2 hidden layers\n",
    "\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "# the underlying assumption is that all hidden layers are of the same siz\n",
    "hidden_layer_size = 150\n",
    "\n",
    "# our data (from tfds) is such that each input is 28x28x1\n",
    "# must flatten a tensor into a vector\n",
    "# 'tf.keras.layers.Flatten(original shape)' transforms (flattens) a tensor into a vector\n",
    "# 'tf.keras.layers.Dense(output size)' takes the inputs, provided to the model and calculates the dot product of the inputs and the weights and adds the bias; also where we can apply an activation function\n",
    "# when creating a classifier, the activation function of the output layer ust transform the vlaues into probabilities\n",
    "# this is done by using 'softmax'\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one of the best choices we've got is the adaptive moment estimation (ADAM)\n",
    "# these strings are NOT case sensitive\n",
    "# TensorFlow employs (3) built-in variations of a cross entropy loss\n",
    "#     1. binary_crossentropy\n",
    "#     2. categorical_crossentropy -> expects that you've one-hot encoded the targets\n",
    "#     3. sparse_categorical_crossentrophy -> applies one-hot encoding\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 32s - loss: 0.2946 - accuracy: 0.9168 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "540/540 - 33s - loss: 0.1168 - accuracy: 0.9650 - val_loss: 0.0948 - val_accuracy: 0.9733\n",
      "Epoch 3/5\n",
      "540/540 - 34s - loss: 0.0786 - accuracy: 0.9762 - val_loss: 0.0779 - val_accuracy: 0.9787\n",
      "Epoch 4/5\n",
      "540/540 - 31s - loss: 0.0618 - accuracy: 0.9801 - val_loss: 0.0576 - val_accuracy: 0.9820\n",
      "Epoch 5/5\n",
      "540/540 - 30s - loss: 0.0475 - accuracy: 0.9848 - val_loss: 0.0530 - val_accuracy: 0.9830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2122e8ab708>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model we built and see it actually works\n",
    "\n",
    "# how many epochs we wish to train for\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# determine the number of validation steps (batch_size)\n",
    "NUM_STEPS = num_validation_samples\n",
    "\n",
    "# fit the model\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), validation_steps=NUM_STEPS, verbose=2)\n",
    "\n",
    "# WHAT HAPPENS INSIDE AN EPOCH\n",
    "#   1. At the beginning, of each epoch, the training loss will be set to 0\n",
    "#   2. The algorithm will iterate over a preset number of batches, all from train_data\n",
    "#   3. The weights and biases will be updated as many times as there are batches\n",
    "#   4. We will get a value for the loss function, indicating how the training is going\n",
    "#   5. We will also see a training accuracy (thanks to 'verbose')\n",
    "#   6. At the end of the epoch, the algorithm will forward proagate the whole validation set\n",
    "#   *** When we reach the maximum number of epochs, the training will be over\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ouput when 'hidden_layer_size = 50':\n",
    "# several lines of output are shown above\n",
    "# loss decreases with each batch, but didn't change too much because after the first epoch, have already had 540 different weight and bias updates  \n",
    "# the accuracy shows in what % of the cases our outputs were equal to the targets\n",
    "# usually keep an eye on the validation loss (or set early stopping mechanisms) to determine whether the model is overfitting\n",
    "# the 'val_accuracy' is the TRUE VALIDATION ACCURACY OF THE MODEL\n",
    "# 97.3% accuracy is great, but let's see if we can do better\n",
    "\n",
    "# Output when 'hidden_layer_size = 100':\n",
    "# 97.6% accuracy; which is a 0.3% increase in accuracy from a 50 hidden layer size\n",
    "\n",
    "# Can we do better than this?\n",
    "\n",
    "# Output when 'hidden_layer_size = 150':\n",
    "# 98.3% accuracy; which is a 0.3% increase from a hidden layer size of 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final accuracy of the model comes from forward propagating the test dataset, NOT the validation\n",
    "# the reason the accuracy of the validation data is not the final, is because we may have overfitted the model\n",
    "# we train on the training data, and then validate on the validation data\n",
    "# that's how we make sure our parameters of the weights and biases don't overfit\n",
    "# once the first model is trained, we fiddle with the hyperparameters:\n",
    "#  1. adjust width of hidden layers\n",
    "#  2. adjust the depth of the learning rate\n",
    "#  3. adjust the batch size\n",
    "#  4. adjust the activation functions for each layer  etc.\n",
    "\n",
    "# the test data set is our reality check that prevents us from overfitting the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.0793 - accuracy: 0.9765\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.08. Test Accuracy: 97.65%\n"
     ]
    }
   ],
   "source": [
    "print('Test Loss: {0:.2f}. Test Accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after we test the model. conceptually, we are no longer allowed to change it because\n",
    "# the test data will no longer be a data set the model has never seen\n",
    "# you would have feedback \n",
    "# main point of the test dataset is to simulate model deployment if we get 50 or 60% testing accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
